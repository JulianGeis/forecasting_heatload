{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MV_GRU_1.ipynb","version":"0.3.2","provenance":[{"file_id":"1Gzrwtc0sOZroNwL5kaUBGq6mHScUzAAa","timestamp":1565460613528}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"W_44bbxNRc4S","colab_type":"code","outputId":"76a30ea7-3b7b-4575-9c64-334103ea8e8b","executionInfo":{"status":"ok","timestamp":1566642541376,"user_tz":-120,"elapsed":33276,"user":{"displayName":"Julian Geis","photoUrl":"","userId":"11665835378505867351"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":74}},"source":["import tensorflow as tf\n","tf.test.gpu_device_name()\n","import os\n","import numpy as np\n","from math import sqrt\n","from numpy import concatenate\n","import datetime as dt\n","import matplotlib.pyplot as plt\n","from pandas import read_csv\n","from pandas import DataFrame\n","from pandas import concat\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import mean_squared_error,mean_absolute_error\n","from tensorflow.python.keras.optimizers import SGD\n","from tensorflow.python.keras.models import Sequential\n","from tensorflow.python.keras.layers import Dense\n","from tensorflow.python.keras.layers import GRU\n","from tensorflow.python.keras.layers import Dropout\n","from tensorflow.python.keras.optimizers import RMSprop\n","from tensorflow.python.keras.optimizers import Adagrad\n","from tensorflow.python.keras.optimizers import Adadelta\n","from tensorflow.python.keras.optimizers import Adam\n","from tensorflow.python.keras.callbacks import EarlyStopping\n","from tensorflow.python.keras.initializers import RandomUniform\n","\n","from google.colab import files\n","uploaded = files.upload()\n","\n","### methods\n","\n","# convert series to supervised learning\n","def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","    n_vars = 1 if type(data) is list else data.shape[1]\n","    df = DataFrame(data)\n","    cols, names = list(), list()\n","    # input sequence (t-n, ... t-1)\n","    for i in range(n_in, 0, -1):\n","        cols.append(df.shift(i))\n","        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n","    # forecast sequence (t, t+1, ... t+n)\n","    for i in range(0, n_out):\n","        cols.append(df[0].shift(-i))\n","        if i == 0:\n","            names += [('var1(t)')]\n","        else:\n","            names += [('var1(t+%d)' % (i))]\n","    # put it all together\n","    agg = concat(cols, axis=1)\n","    agg.columns = names\n","    # drop rows with NaN values\n","    if dropnan:\n","        agg.dropna(inplace=True)\n","    return agg\n","  \n","def mean_absolute_percentage_error(y_true, y_pred):\n","    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-34c8981f-8bbb-43ee-ab2b-593baf65aa48\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-34c8981f-8bbb-43ee-ab2b-593baf65aa48\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving data_dummies_index to data_dummies_index\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h55tUZ66T-Zt","colab_type":"code","outputId":"51fad306-ec8d-4510-d7eb-cb4491a8825c","executionInfo":{"status":"ok","timestamp":1566648941351,"user_tz":-120,"elapsed":5228826,"user":{"displayName":"Julian Geis","photoUrl":"","userId":"11665835378505867351"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#for h in range(1,3,1):\n","#for i in range(3,4,1):\n","  \n","  # Observing script runtime\n","  startTime = dt.datetime.now()  # observe script running time\n","\n","  data = pd.read_csv('data_dummies_index', header=0, index_col=0, date_parser=pd.to_datetime)\n","  data['temp']= data['temp'].shift(-72)    # shift temperature 72h in past to obtain 3 day ahead temperature forecast (temp from 2014-01-04 00:00:00 is switched to 2014-01-01 00:00:00\n","  data.rename(columns={'temp':'temp_72'}, inplace=True)\n","  data['hour']= data['hour'] + 1\n","\n","  # 1.3 Scaling\n","  scaling = 2 # 0 = No scaling | 1 = MinMax Scaling | 2 = Z-Score\n","\n","  # Scaling the data between min and max: x_scaled = (x - min(x)) / ( max(x) - min(x)) only with parameters calculated from the train set\n","  if (scaling == 1):\n","      min, max = 0, 1\n","      min_load_train, max_load_train = max(data['load']['2014-01-01 00:00:00':'2015-12-31 23:00:00']), min(data['load']['2014-01-01 00:00:00':'2015-12-31 23:00:00'])\n","      data['load'] = ((data['load'] - min_load_train) / (max_load_train - min_load_train)) * (max - min) + min    # Inverse scaling: x = (x_scaled - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n","      min_temp_train, max_temp_train = max(data['temp_72']['2014-01-01 00:00:00':'2015-12-31 23:00:00']), min( data['temp_72']['2014-01-01 00:00:00':'2015-12-31 23:00:00'])\n","      data['temp_72'] = ((data['temp_72'] - min_temp_train) / (max_temp_train - min_temp_train)) * (max - min) + min  # Inverse scaling: x = (x_scaled - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n","\n","  # Scaling the data as folllows:  x_scaled = (x - mean(x)) / st(x) only with parameters calculated from the train set\n","  if (scaling == 2):\n","      mean_load_train, sd_load_train = np.mean(data['load']['2014-01-01 00:00:00':'2015-12-31 23:00:00']), np.sqrt(np.var(data['load']['2014-01-01 00:00:00':'2015-12-31 23:00:00']))\n","      data['load'] = (data['load'] - mean_load_train) / sd_load_train   # Inverse scaling: x = x_scaled * sd_load_train + mean_load_train\n","      mean_temp_train, sd_temp_train = np.mean(data['temp_72']['2014-01-01 00:00:00':'2015-12-31 23:00:00']), np.sqrt(np.var(data['temp_72']['2014-01-01 00:00:00':'2015-12-31 23:00:00']))\n","      data['temp_72'] = (data['temp_72'] - mean_temp_train) / sd_temp_train  # Inverse scaling: x = x_scaled * sd_load_train + mean_load_train\n","      mean_hour_train, sd_hour_train = np.mean(data['hour']['2014-01-01 00:00:00':'2015-12-31 23:00:00']), np.sqrt(np.var(data['hour']['2014-01-01 00:00:00':'2015-12-31 23:00:00']))\n","      data['hour'] = (data['hour'] - mean_hour_train) / sd_hour_train  # Inverse scaling: x = x_scaled * sd_load_train + mean_load_train\n","\n","\n","  data = data[['load', 'temp_72', 'hour']]     # chose columns to include in forecast (26304, 3) e.g. data = data.drop(['const','weekday','hour'], axis=1)  from 2014-01-01 00:00:00 until 2016-12-31 23:00:00\n","\n","  # ensure all data is float\n","  values = data.values.astype('float32')\n","\n","  # specify the number of lag hours\n","  train_n = 24*5\n","  predict_n = 24*3\n","  n_features = data.shape[1]   # number of input variables\n","\n","  # data preparation\n","  # 1: obtain percentage of training set as validation set and then shuffle during training\n","  # 2: take only every k th sample of training set and obtain a percentage of the randomised training set as validation data \n","  preparation = 1 # h \n","  p = 0.25 # perceentage of the training data as validation\n","  k = 1\n","\n","  if (preparation == 1):\n","    # split into train, validation and test sets\n","    train = series_to_supervised(values[ :round(365*2*24*(1-p)), :], train_n, predict_n).values\n","    val =   series_to_supervised(values[round(365*2*24*(1-p)):365*2*24, :], train_n, predict_n).values\n","    test =  series_to_supervised(values[365*2*24: , : ], train_n, predict_n).values\n","\n","  if (preparation == 2):\n","    train_val = np.array(pd.DataFrame(series_to_supervised(values[:365*2*24, :], train_n, predict_n)).iloc[::k, :])\n","    np.random.shuffle(train_val)\n","    train = train_val[:round(len(train_val)*(1-p)), : ]\n","    val = train_val[round(len(train_val)*(1-p)): , : ]\n","    test =  np.array(pd.DataFrame(series_to_supervised(values[365*2*24:, :], train_n, predict_n)).iloc[::k, :])\n","\n","\n","  # split into input and outputs\n","  input_cols = train_n * n_features    # columns of input tensor\n","  X_train, y_train = train[:, :input_cols], train[:, -predict_n:]  # (training samples, columns of input tensor) | (training samples, number of periods to predict)\n","  X_val, y_val = val[:, :input_cols], val[:, -predict_n:]\n","  X_test, y_test = test[:, :input_cols], test[:, -predict_n:]  # (testing samples, columns of input tensor) | (testing samples samples, number of periods to predict)\n","\n","  # Selecting only every j-th sample\n","  j = 1   # j = 24 means you only consider every 24th sample\n","  X_train, y_train = np.array(pd.DataFrame(X_train).iloc[::j, :]), np.array(pd.DataFrame(y_train).iloc[::j, :])\n","  X_test, y_test = np.array(pd.DataFrame(X_test).iloc[::j, :]), np.array(pd.DataFrame(y_test).iloc[::j, :])\n","\n","  # reshape input to be 3D [samples, timesteps, features]\n","  X_train = X_train.reshape((X_train.shape[0], train_n, n_features))  # (training samples, number of periods to train, number of input variables)\n","  X_test = X_test.reshape((X_test.shape[0], train_n, n_features)) # # (testing samples, number of periods to train, number of input variables)\n","  X_val = X_val.reshape((X_val.shape[0], train_n, n_features))  # (training samples, number of periods to train, number of input variables)\n","\n","  ##### 02 Network selection\n","  hiddenNeurons = 200\n","\n","  # architecture\n","  mv_gru = Sequential()\n","  mv_gru.add(GRU(units=hiddenNeurons, return_sequences=False, activation='tanh', input_shape=(X_train.shape[1], X_train.shape[2])))\n","  mv_gru.add(Dropout(0.2))\n","\n","  mv_gru.add(Dense(units=predict_n, activation='linear', kernel_initializer=RandomUniform(minval=-0.05, maxval=0.05)))\n","\n","  # optimizers:\n","  sgd=SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False, clipnorm=1.0)  # default: SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n","  rmsprop=RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0) # default:RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n","  adagrad=Adagrad(lr=0.01, epsilon=None, decay=0.0)    # default: Adagrad(lr=0.01, epsilon=None, decay=0.0)\n","  adadelta=Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)     # default: Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n","  adam=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999,clipnorm=1.0, epsilon=None, decay=0.0, amsgrad=False)   # default=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","\n","  # compilation\n","  mv_gru.compile(optimizer=adam, loss='mse') # working: 'adam' (relu), 'rmsprop(lr=0.001)'   | not working: sgd (just with tanh)\n","\n","  # patient early stopping\n","  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n","\n","  # fit model\n","  history = mv_gru.fit(X_train, y_train, batch_size = 200, epochs=1000, verbose=2,\n","                        validation_data=(X_val, y_val),callbacks=[es], shuffle=True)\n","  \n","  print( 'Hidden Neurons: ' + str(hiddenNeurons))\n","  print( 'Preparation: ' + str(preparation))\n","  print( 'p: ' + str(p))\n","\n","  # prediction\n","  if (scaling == 0):\n","      yhat = mv_gru.predict(X_test)\n","      yfit = mv_gru.predict(X_train)  # fitted values\n","      yval = mv_gru.predict(X_val)  # fitted values\n","\n","  if (scaling == 1):\n","      yhat_s = mv_gru.predict(X_test)\n","      yhat = (yhat_s - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n","      y_test = (y_test - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n","\n","      yval_s = mv_gru.predict(X_val)\n","      yval = (yval_s - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n","      y_val = (y_val - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n","\n","      yfit_s = mv_gru.predict(X_train) \n","      yfit = (yfit_s - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n","      y_train = (y_train - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n","\n","  if (scaling == 2):\n","      yhat_s = mv_gru.predict(X_test)\n","      yhat = yhat_s * sd_load_train + mean_load_train\n","      y_test = y_test * sd_load_train + mean_load_train\n","\n","      yfit_s = mv_gru.predict(X_train)\n","      yfit = yfit_s * sd_load_train + mean_load_train\n","      y_train = y_train  * sd_load_train + mean_load_train\n","\n","      yval_s = mv_gru.predict(X_val)\n","      yval = yval_s * sd_load_train + mean_load_train\n","      y_val = y_val  * sd_load_train + mean_load_train\n","\n","  # evaluation measures\n","  rmspe_overall = np.sqrt(mean_squared_error(y_test,yhat)); print('rmspe_overall: ' + str(rmspe_overall))\n","  rmse_validation_overall = np.sqrt(mean_squared_error(y_val,yval)); print('rmse_validation_all: ' + str(rmse_validation_overall))\n","  rmse_fitted_overall = np.sqrt(mean_squared_error(y_train,yfit)); print('rmse_fitted_all: ' + str(rmse_fitted_overall))\n","  mae_overall = mean_absolute_error(y_test,yhat); print('mae_overall: ' + str(mae_overall))\n","  mape_overall = mean_absolute_percentage_error(y_test,yhat); print('mape_overall: ' + str(mape_overall))\n","\n","  print('elapsed time: ', dt.datetime.now() - startTime)  # observe script running time\n","\n","    ## Download model\n","    #mv_gru.save('mv_gru' + str(np.round(rmspe_overall,2)) + '.h5') \n","    #from google.colab import files\n","    #files.download('mv_gru' + str(np.round(rmspe_overall,2)) + '.h5')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Train on 12949 samples, validate on 4189 samples\n","Epoch 1/1000\n","12949/12949 - 67s - loss: 0.9212 - val_loss: 0.3874\n","Epoch 2/1000\n","12949/12949 - 65s - loss: 0.2222 - val_loss: 0.0646\n","Epoch 3/1000\n","12949/12949 - 65s - loss: 0.1135 - val_loss: 0.0633\n","Epoch 4/1000\n","12949/12949 - 65s - loss: 0.0993 - val_loss: 0.0655\n","Epoch 5/1000\n","12949/12949 - 65s - loss: 0.0912 - val_loss: 0.0670\n","Epoch 6/1000\n","12949/12949 - 66s - loss: 0.0882 - val_loss: 0.0664\n","Epoch 7/1000\n","12949/12949 - 65s - loss: 0.0861 - val_loss: 0.0644\n","Epoch 8/1000\n","12949/12949 - 65s - loss: 0.0849 - val_loss: 0.0627\n","Epoch 9/1000\n","12949/12949 - 65s - loss: 0.0834 - val_loss: 0.0612\n","Epoch 10/1000\n","12949/12949 - 66s - loss: 0.0819 - val_loss: 0.0605\n","Epoch 11/1000\n","12949/12949 - 66s - loss: 0.0806 - val_loss: 0.0589\n","Epoch 12/1000\n","12949/12949 - 66s - loss: 0.0794 - val_loss: 0.0580\n","Epoch 13/1000\n","12949/12949 - 65s - loss: 0.0784 - val_loss: 0.0570\n","Epoch 14/1000\n","12949/12949 - 65s - loss: 0.0772 - val_loss: 0.0556\n","Epoch 15/1000\n","12949/12949 - 66s - loss: 0.0759 - val_loss: 0.0553\n","Epoch 16/1000\n","12949/12949 - 65s - loss: 0.0751 - val_loss: 0.0541\n","Epoch 17/1000\n","12949/12949 - 65s - loss: 0.0737 - val_loss: 0.0531\n","Epoch 18/1000\n","12949/12949 - 65s - loss: 0.0725 - val_loss: 0.0520\n","Epoch 19/1000\n","12949/12949 - 65s - loss: 0.0711 - val_loss: 0.0501\n","Epoch 20/1000\n","12949/12949 - 66s - loss: 0.0696 - val_loss: 0.0488\n","Epoch 21/1000\n","12949/12949 - 67s - loss: 0.0676 - val_loss: 0.0470\n","Epoch 22/1000\n","12949/12949 - 67s - loss: 0.0654 - val_loss: 0.0460\n","Epoch 23/1000\n","12949/12949 - 67s - loss: 0.0638 - val_loss: 0.0434\n","Epoch 24/1000\n","12949/12949 - 67s - loss: 0.0618 - val_loss: 0.0422\n","Epoch 25/1000\n","12949/12949 - 67s - loss: 0.0602 - val_loss: 0.0403\n","Epoch 26/1000\n","12949/12949 - 67s - loss: 0.0590 - val_loss: 0.0393\n","Epoch 27/1000\n","12949/12949 - 67s - loss: 0.0576 - val_loss: 0.0379\n","Epoch 28/1000\n","12949/12949 - 67s - loss: 0.0568 - val_loss: 0.0380\n","Epoch 29/1000\n","12949/12949 - 67s - loss: 0.0557 - val_loss: 0.0389\n","Epoch 30/1000\n","12949/12949 - 65s - loss: 0.0548 - val_loss: 0.0371\n","Epoch 31/1000\n","12949/12949 - 65s - loss: 0.0539 - val_loss: 0.0373\n","Epoch 32/1000\n","12949/12949 - 65s - loss: 0.0533 - val_loss: 0.0357\n","Epoch 33/1000\n","12949/12949 - 66s - loss: 0.0521 - val_loss: 0.0360\n","Epoch 34/1000\n","12949/12949 - 66s - loss: 0.0514 - val_loss: 0.0357\n","Epoch 35/1000\n","12949/12949 - 66s - loss: 0.0508 - val_loss: 0.0361\n","Epoch 36/1000\n","12949/12949 - 66s - loss: 0.0500 - val_loss: 0.0349\n","Epoch 37/1000\n","12949/12949 - 66s - loss: 0.0493 - val_loss: 0.0353\n","Epoch 38/1000\n","12949/12949 - 66s - loss: 0.0485 - val_loss: 0.0356\n","Epoch 39/1000\n","12949/12949 - 66s - loss: 0.0480 - val_loss: 0.0336\n","Epoch 40/1000\n","12949/12949 - 66s - loss: 0.0475 - val_loss: 0.0324\n","Epoch 41/1000\n","12949/12949 - 66s - loss: 0.0467 - val_loss: 0.0333\n","Epoch 42/1000\n","12949/12949 - 66s - loss: 0.0462 - val_loss: 0.0329\n","Epoch 43/1000\n","12949/12949 - 66s - loss: 0.0458 - val_loss: 0.0325\n","Epoch 44/1000\n","12949/12949 - 66s - loss: 0.0449 - val_loss: 0.0331\n","Epoch 45/1000\n","12949/12949 - 66s - loss: 0.0442 - val_loss: 0.0319\n","Epoch 46/1000\n","12949/12949 - 66s - loss: 0.0435 - val_loss: 0.0316\n","Epoch 47/1000\n","12949/12949 - 66s - loss: 0.0430 - val_loss: 0.0310\n","Epoch 48/1000\n","12949/12949 - 67s - loss: 0.0425 - val_loss: 0.0311\n","Epoch 49/1000\n","12949/12949 - 67s - loss: 0.0423 - val_loss: 0.0306\n","Epoch 50/1000\n","12949/12949 - 68s - loss: 0.0415 - val_loss: 0.0300\n","Epoch 51/1000\n","12949/12949 - 67s - loss: 0.0409 - val_loss: 0.0292\n","Epoch 52/1000\n","12949/12949 - 68s - loss: 0.0408 - val_loss: 0.0299\n","Epoch 53/1000\n","12949/12949 - 68s - loss: 0.0402 - val_loss: 0.0298\n","Epoch 54/1000\n","12949/12949 - 67s - loss: 0.0400 - val_loss: 0.0298\n","Epoch 55/1000\n","12949/12949 - 67s - loss: 0.0398 - val_loss: 0.0294\n","Epoch 56/1000\n","12949/12949 - 67s - loss: 0.0395 - val_loss: 0.0291\n","Epoch 57/1000\n","12949/12949 - 67s - loss: 0.0391 - val_loss: 0.0286\n","Epoch 58/1000\n","12949/12949 - 67s - loss: 0.0386 - val_loss: 0.0290\n","Epoch 59/1000\n","12949/12949 - 67s - loss: 0.0385 - val_loss: 0.0282\n","Epoch 60/1000\n","12949/12949 - 68s - loss: 0.0382 - val_loss: 0.0281\n","Epoch 61/1000\n","12949/12949 - 67s - loss: 0.0382 - val_loss: 0.0289\n","Epoch 62/1000\n","12949/12949 - 68s - loss: 0.0376 - val_loss: 0.0282\n","Epoch 63/1000\n","12949/12949 - 68s - loss: 0.0377 - val_loss: 0.0288\n","Epoch 64/1000\n","12949/12949 - 67s - loss: 0.0375 - val_loss: 0.0290\n","Epoch 65/1000\n","12949/12949 - 68s - loss: 0.0375 - val_loss: 0.0285\n","Epoch 66/1000\n","12949/12949 - 68s - loss: 0.0369 - val_loss: 0.0294\n","Epoch 67/1000\n","12949/12949 - 68s - loss: 0.0371 - val_loss: 0.0290\n","Epoch 68/1000\n","12949/12949 - 67s - loss: 0.0367 - val_loss: 0.0279\n","Epoch 69/1000\n","12949/12949 - 68s - loss: 0.0365 - val_loss: 0.0280\n","Epoch 70/1000\n","12949/12949 - 67s - loss: 0.0365 - val_loss: 0.0289\n","Epoch 71/1000\n","12949/12949 - 68s - loss: 0.0362 - val_loss: 0.0297\n","Epoch 72/1000\n","12949/12949 - 67s - loss: 0.0363 - val_loss: 0.0292\n","Epoch 73/1000\n","12949/12949 - 67s - loss: 0.0359 - val_loss: 0.0288\n","Epoch 74/1000\n","12949/12949 - 67s - loss: 0.0357 - val_loss: 0.0284\n","Epoch 75/1000\n","12949/12949 - 68s - loss: 0.0356 - val_loss: 0.0280\n","Epoch 76/1000\n","12949/12949 - 68s - loss: 0.0354 - val_loss: 0.0282\n","Epoch 77/1000\n","12949/12949 - 67s - loss: 0.0351 - val_loss: 0.0287\n","Epoch 78/1000\n","12949/12949 - 67s - loss: 0.0350 - val_loss: 0.0281\n","Epoch 00078: early stopping\n","Hidden Neurons: 200\n","Preparation: 1\n","p: 0.25\n","rmspe_overall: 13.687686\n","rmse_validation_all: 11.8900795\n","rmse_fitted_all: 12.525849\n","mae_overall: 9.806673\n","mape_overall: 8.088022470474243\n","elapsed time:  1:27:08.005818\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E4bY7KfNqU6h","colab_type":"code","colab":{}},"source":["# Download y_test and y_hat\n","\n","pd.DataFrame(yhat).to_csv('y_hat__basic_final') \n","from google.colab import files\n","files.download('y_hat__basic_final')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ef5CAS5mnkiA","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6e9FIn04rJEu","colab_type":"code","colab":{}},"source":["# Download model\n","mv_gru.save('mv_gru_basic_final.h5') \n","from google.colab import files\n","files.download(\"mv_gru_basic_final.h5\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xAhDZ5-IlLpS","colab_type":"code","colab":{}},"source":["!nvidia-smi"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewSjxtDG0MYg","colab_type":"code","colab":{}},"source":["#### Load model\n","from google.colab import files\n","uploaded = files.upload()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8MGlRQyFKZxv","colab_type":"code","colab":{}},"source":["# Download y_test and y_hat\n","pd.DataFrame(y_test).to_csv('y_test3__basic_final') \n","from google.colab import files\n","files.download('y_test3__basic_final')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n7hUqGBs0uQO","colab_type":"code","colab":{}},"source":["from tensorflow.python.keras.models import load_model\n","mv_lstm_load = load_model('mv_lstm1272.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VBLYMNgw1r8b","colab_type":"code","colab":{}},"source":["mv_lstm_load.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oFR8Lr1_zTGk","colab_type":"code","colab":{}},"source":["##### 03 Evaluation\n","\n","# plot history\n","plt.plot(history.history['loss'][10:], label='train')\n","plt.plot(history.history['val_loss'][10:], label='test')\n","plt.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXrsVqEXzrtO","colab_type":"code","colab":{}},"source":["# plot prediction\n"," for i in range(0, len(y_test), 24*7*4):\n","     plt.figure(i)\n","     plt.plot(yhat[i], label='prediction')\n","     plt.plot(y_test[i], label ='observation')\n","     plt.title('prediction of week ' + str((round(i/(24*7)+1,0))))\n","     plt.xlabel('hour');plt.ylabel('MW');plt.legend()\n","     plt.legend()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmP1VqM3zs43","colab_type":"code","colab":{}},"source":["# plot \"fitted values\"\n"," for i in range(0, len(y_train), 24*7*4):\n","     plt.figure(i+10)\n","     plt.plot(yfit[i], label='fitted values')\n","     plt.plot(y_train[i], label ='observation')\n","     plt.title('fitted values of week ' + str((round(i/(24*7)+1,0))))\n","     plt.xlabel('hour');plt.ylabel('MW');plt.legend()\n","     plt.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_QxtO3NOzpVq","colab_type":"code","colab":{}},"source":["# Download model\n","mv_lstm.save('mv_lstm1400.h5') \n","from google.colab import files\n","files.download(\"mv_lstm1400.h5\")"],"execution_count":0,"outputs":[]}]}