{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulianGeis/forecasting_heatload/blob/master/GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_44bbxNRc4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "import os\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
        "from tensorflow.python.keras.optimizers import SGD\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import GRU\n",
        "from tensorflow.python.keras.layers import Dropout\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "from tensorflow.python.keras.optimizers import Adagrad\n",
        "from tensorflow.python.keras.optimizers import Adadelta\n",
        "from tensorflow.python.keras.optimizers import Adam\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.python.keras.initializers import RandomUniform\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "### methods\n",
        "\n",
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df[0].shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var1(t)')]\n",
        "        else:\n",
        "            names += [('var1(t+%d)' % (i))]\n",
        "    # put it all together\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg\n",
        "  \n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h55tUZ66T-Zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Observing script runtime\n",
        "startTime = dt.datetime.now()  # observe script running time\n",
        "\n",
        "data = pd.read_csv('data_dummies_index', header=0, index_col=0, date_parser=pd.to_datetime)\n",
        "data['temp']= data['temp'].shift(-72)    # shift temperature 72h in past to obtain 3 day ahead temperature forecast (temp from 2014-01-04 00:00:00 is switched to 2014-01-01 00:00:00\n",
        "data.rename(columns={'temp':'temp_72'}, inplace=True)\n",
        "data['hour']= data['hour'] + 1\n",
        "\n",
        "# 1.3 Scaling\n",
        "scaling = 2 # 0 = No scaling | 1 = MinMax Scaling | 2 = Z-Score\n",
        "\n",
        "# Scaling the data between min and max: x_scaled = (x - min(x)) / ( max(x) - min(x)) only with parameters calculated from the train set\n",
        "if (scaling == 1):\n",
        "    min, max = 0, 1\n",
        "    min_load_train, max_load_train = max(data['load']['2014-01-01 00:00:00':'2015-12-31 23:00:00']), min(data['load']['2014-01-01 00:00:00':'2015-12-31 23:00:00'])\n",
        "    data['load'] = ((data['load'] - min_load_train) / (max_load_train - min_load_train)) * (max - min) + min    # Inverse scaling: x = (x_scaled - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "    min_temp_train, max_temp_train = max(data['temp_72']['2014-01-01 00:00:00':'2015-12-31 23:00:00']), min( data['temp_72']['2014-01-01 00:00:00':'2015-12-31 23:00:00'])\n",
        "    data['temp_72'] = ((data['temp_72'] - min_temp_train) / (max_temp_train - min_temp_train)) * (max - min) + min  # Inverse scaling: x = (x_scaled - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "\n",
        "# Scaling the data as folllows:  x_scaled = (x - mean(x)) / st(x) only with parameters calculated from the train set\n",
        "if (scaling == 2):\n",
        "    mean_load_train, sd_load_train = np.mean(data['load']['2014-01-01 00:00:00':'2015-12-31 23:00:00']), np.sqrt(np.var(data['load']['2014-01-01 00:00:00':'2015-12-31 23:00:00']))\n",
        "    data['load'] = (data['load'] - mean_load_train) / sd_load_train   # Inverse scaling: x = x_scaled * sd_load_train + mean_load_train\n",
        "    mean_temp_train, sd_temp_train = np.mean(data['temp_72']['2014-01-01 00:00:00':'2015-12-31 23:00:00']), np.sqrt(np.var(data['temp_72']['2014-01-01 00:00:00':'2015-12-31 23:00:00']))\n",
        "    data['temp_72'] = (data['temp_72'] - mean_temp_train) / sd_temp_train  # Inverse scaling: x = x_scaled * sd_load_train + mean_load_train\n",
        "    mean_hour_train, sd_hour_train = np.mean(data['hour']['2014-01-01 00:00:00':'2015-12-31 23:00:00']), np.sqrt(np.var(data['hour']['2014-01-01 00:00:00':'2015-12-31 23:00:00']))\n",
        "    data['hour'] = (data['hour'] - mean_hour_train) / sd_hour_train  # Inverse scaling: x = x_scaled * sd_load_train + mean_load_train\n",
        "\n",
        "\n",
        "data = data[['load', 'temp_72', 'hour']]     # chose columns to include in forecast (26304, 3) e.g. data = data.drop(['const','weekday','hour'], axis=1)  from 2014-01-01 00:00:00 until 2016-12-31 23:00:00\n",
        "\n",
        "# ensure all data is float\n",
        "values = data.values.astype('float32')\n",
        "\n",
        "# specify the number of lag hours\n",
        "train_n = 24*5\n",
        "predict_n = 24*3\n",
        "n_features = data.shape[1]   # number of input variables\n",
        "\n",
        "# data preparation\n",
        "# 1: obtain percentage of training set as validation set and then shuffle during training\n",
        "# 2: take only every k th sample of training set and obtain a percentage of the randomised training set as validation data \n",
        "preparation = 1 # h \n",
        "p = 0.25 # perceentage of the training data as validation\n",
        "k = 1\n",
        "\n",
        "if (preparation == 1):\n",
        "  # split into train, validation and test sets\n",
        "  train = series_to_supervised(values[ :round(365*2*24*(1-p)), :], train_n, predict_n).values\n",
        "  val =   series_to_supervised(values[round(365*2*24*(1-p)):365*2*24, :], train_n, predict_n).values\n",
        "  test =  series_to_supervised(values[365*2*24: , : ], train_n, predict_n).values\n",
        "\n",
        "if (preparation == 2):\n",
        "  train_val = np.array(pd.DataFrame(series_to_supervised(values[:365*2*24, :], train_n, predict_n)).iloc[::k, :])\n",
        "  np.random.shuffle(train_val)\n",
        "  train = train_val[:round(len(train_val)*(1-p)), : ]\n",
        "  val = train_val[round(len(train_val)*(1-p)): , : ]\n",
        "  test =  np.array(pd.DataFrame(series_to_supervised(values[365*2*24:, :], train_n, predict_n)).iloc[::k, :])\n",
        "\n",
        "\n",
        "# split into input and outputs\n",
        "input_cols = train_n * n_features    # columns of input tensor\n",
        "X_train, y_train = train[:, :input_cols], train[:, -predict_n:]  # (training samples, columns of input tensor) | (training samples, number of periods to predict)\n",
        "X_val, y_val = val[:, :input_cols], val[:, -predict_n:]\n",
        "X_test, y_test = test[:, :input_cols], test[:, -predict_n:]  # (testing samples, columns of input tensor) | (testing samples samples, number of periods to predict)\n",
        "\n",
        "# Selecting only every j-th sample\n",
        "j = 1   # j = 24 means you only consider every 24th sample\n",
        "X_train, y_train = np.array(pd.DataFrame(X_train).iloc[::j, :]), np.array(pd.DataFrame(y_train).iloc[::j, :])\n",
        "X_test, y_test = np.array(pd.DataFrame(X_test).iloc[::j, :]), np.array(pd.DataFrame(y_test).iloc[::j, :])\n",
        "\n",
        "# reshape input to be 3D [samples, timesteps, features]\n",
        "X_train = X_train.reshape((X_train.shape[0], train_n, n_features))  # (training samples, number of periods to train, number of input variables)\n",
        "X_test = X_test.reshape((X_test.shape[0], train_n, n_features)) # # (testing samples, number of periods to train, number of input variables)\n",
        "X_val = X_val.reshape((X_val.shape[0], train_n, n_features))  # (training samples, number of periods to train, number of input variables)\n",
        "\n",
        "##### 02 Network selection\n",
        "hiddenNeurons = 200\n",
        "\n",
        "# architecture\n",
        "mv_gru = Sequential()\n",
        "mv_gru.add(GRU(units=hiddenNeurons, return_sequences=False, activation='tanh', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "mv_gru.add(Dropout(0.2))\n",
        "\n",
        "mv_gru.add(Dense(units=predict_n, activation='linear', kernel_initializer=RandomUniform(minval=-0.05, maxval=0.05)))\n",
        "\n",
        "# optimizers:\n",
        "sgd=SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False, clipnorm=1.0)  # default: SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "rmsprop=RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0) # default:RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "adagrad=Adagrad(lr=0.01, epsilon=None, decay=0.0)    # default: Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
        "adadelta=Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)     # default: Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
        "adam=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999,clipnorm=1.0, epsilon=None, decay=0.0, amsgrad=False)   # default=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "# compilation\n",
        "mv_gru.compile(optimizer=adam, loss='mse') # working: 'adam' (relu), 'rmsprop(lr=0.001)'   | not working: sgd (just with tanh)\n",
        "\n",
        "# patient early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "# fit model\n",
        "history = mv_gru.fit(X_train, y_train, batch_size = 200, epochs=1000, verbose=2,\n",
        "                      validation_data=(X_val, y_val),callbacks=[es], shuffle=True)\n",
        "\n",
        "print( 'Hidden Neurons: ' + str(hiddenNeurons))\n",
        "print( 'Preparation: ' + str(preparation))\n",
        "print( 'p: ' + str(p))\n",
        "\n",
        "# prediction\n",
        "if (scaling == 0):\n",
        "    yhat = mv_gru.predict(X_test)\n",
        "    yfit = mv_gru.predict(X_train)  # fitted values\n",
        "    yval = mv_gru.predict(X_val)  # fitted values\n",
        "\n",
        "if (scaling == 1):\n",
        "    yhat_s = mv_gru.predict(X_test)\n",
        "    yhat = (yhat_s - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "    y_test = (y_test - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "\n",
        "    yval_s = mv_gru.predict(X_val)\n",
        "    yval = (yval_s - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "    y_val = (y_val - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "\n",
        "    yfit_s = mv_gru.predict(X_train) \n",
        "    yfit = (yfit_s - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "    y_train = (y_train - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "\n",
        "if (scaling == 2):\n",
        "    yhat_s = mv_gru.predict(X_test)\n",
        "    yhat = yhat_s * sd_load_train + mean_load_train\n",
        "    y_test = y_test * sd_load_train + mean_load_train\n",
        "\n",
        "    yfit_s = mv_gru.predict(X_train)\n",
        "    yfit = yfit_s * sd_load_train + mean_load_train\n",
        "    y_train = y_train  * sd_load_train + mean_load_train\n",
        "\n",
        "    yval_s = mv_gru.predict(X_val)\n",
        "    yval = yval_s * sd_load_train + mean_load_train\n",
        "    y_val = y_val  * sd_load_train + mean_load_train\n",
        "\n",
        "# evaluation measures\n",
        "rmspe_overall = np.sqrt(mean_squared_error(y_test,yhat)); print('rmspe_overall: ' + str(rmspe_overall))\n",
        "rmse_validation_overall = np.sqrt(mean_squared_error(y_val,yval)); print('rmse_validation_all: ' + str(rmse_validation_overall))\n",
        "rmse_fitted_overall = np.sqrt(mean_squared_error(y_train,yfit)); print('rmse_fitted_all: ' + str(rmse_fitted_overall))\n",
        "mae_overall = mean_absolute_error(y_test,yhat); print('mae_overall: ' + str(mae_overall))\n",
        "mape_overall = mean_absolute_percentage_error(y_test,yhat); print('mape_overall: ' + str(mape_overall))\n",
        "\n",
        "print('elapsed time: ', dt.datetime.now() - startTime)  # observe script running time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFR8Lr1_zTGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### 03 Evaluation\n",
        "\n",
        "# plot history\n",
        "plt.plot(history.history['loss'][10:], label='train')\n",
        "plt.plot(history.history['val_loss'][10:], label='test')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXrsVqEXzrtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot prediction\n",
        " for i in range(0, len(y_test), 24*7*4):\n",
        "     plt.figure(i)\n",
        "     plt.plot(yhat[i], label='prediction')\n",
        "     plt.plot(y_test[i], label ='observation')\n",
        "     plt.title('prediction of week ' + str((round(i/(24*7)+1,0))))\n",
        "     plt.xlabel('hour');plt.ylabel('MW');plt.legend()\n",
        "     plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmP1VqM3zs43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot \"fitted values\"\n",
        " for i in range(0, len(y_train), 24*7*4):\n",
        "     plt.figure(i+10)\n",
        "     plt.plot(yfit[i], label='fitted values')\n",
        "     plt.plot(y_train[i], label ='observation')\n",
        "     plt.title('fitted values of week ' + str((round(i/(24*7)+1,0))))\n",
        "     plt.xlabel('hour');plt.ylabel('MW');plt.legend()\n",
        "     plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}