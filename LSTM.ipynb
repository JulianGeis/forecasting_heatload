{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulianGeis/forecasting_heatload/blob/master/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_44bbxNRc4S",
        "colab_type": "code",
        "outputId": "78fa3c67-d535-480b-ca70-fa86256182bf",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "import os\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "import pandas as pd\n",
        "import random as rd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.python.keras.optimizers import SGD\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import LSTM\n",
        "from tensorflow.python.keras.layers import MaxPooling1D\n",
        "from tensorflow.python.keras.layers import Flatten\n",
        "from tensorflow.python.keras.layers import Dropout\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "from tensorflow.python.keras.optimizers import Adagrad\n",
        "from tensorflow.python.keras.optimizers import Adadelta\n",
        "from tensorflow.python.keras.optimizers import Adam\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.python.keras.initializers import RandomUniform\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "### methods\n",
        "\n",
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df[0].shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var1(t)')]\n",
        "        else:\n",
        "            names += [('var1(t+%d)' % (i))]\n",
        "    # put it all together\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8685903e-9419-4b0d-b40b-241b934ee4cd\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8685903e-9419-4b0d-b40b-241b934ee4cd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data_dummies_index to data_dummies_index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h55tUZ66T-Zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Observing script runtime\n",
        "startScript = dt.datetime.now()  # observe script running time\n",
        "\n",
        "optimising = 1  # default model vs. hyper-parameter optimising\n",
        "runs = 2\n",
        "result = pd.DataFrame(index=range(runs), columns= ['rmse_all','rmse_val','rmse_fit','train_days', 'batch_size','train_algo','lr', 'decay','patience', 'val_split', 'activation',\n",
        "                                                   'duration','scaling','regularisation', 'dropout', 'epochs'])\n",
        "\n",
        "for i in range(0,runs,1):\n",
        "  print('run: ' +str(i+1))\n",
        "  \n",
        "  data = pd.read_csv('data_dummies_index', header=0, index_col=0, date_parser=pd.to_datetime)\n",
        "  data['temp']= data['temp'].shift(-72)    # shift temperature 72h in past to obtain 3 day ahead temperature forecast (temp from 2014-01-04 00:00:00 is switched to 2014-01-01 00:00:00\n",
        "  data.rename(columns={'temp':'temp_72'}, inplace=True)\n",
        "  \n",
        "  # Observing runtime of single net\n",
        "  startNet = dt.datetime.now()  # observe script running time\n",
        "  \n",
        "  if (optimising == 0):\n",
        "    scaling = 2   # 0 = No scaling | 1 = MinMax Scaling | 2 = Z-Score \n",
        "    preparation = 1\n",
        "    train_days = 5\n",
        "    train_algo = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "    val_split = 0.25\n",
        "    activation = 'sigmoid'\n",
        "    patience = 20\n",
        "    batch_size = 200\n",
        "    decay = 0   # new learning rate = old learning rate / (1 + decay * iterations)) with iterations are the number of individual algo stepts not epochs\n",
        "    regularisation = 0\n",
        "    dropout = 0.1\n",
        "    \n",
        "  if (optimising == 1):\n",
        "    \n",
        "    # Hyper-parameter optimising:   \n",
        "    lr_sgd_rms = rd.choice([0.0001, 0.001, 0.01, 0.1]) # default = 0.001\n",
        "    lr_adagrad = rd.choice([0.001, 0.01, 0.1, 1])   # default = 0.01\n",
        "    lr_adam = rd.choice([0.00001, 0.0001, 0.001, 0.01]) # default = 0.0001\n",
        "    decay = rd.choice([0,0.0001,0.001, 0.01])   # rate of decay for the learning rate (next epoch will be started using the current learning rate divided by the decay parameter)\n",
        "\n",
        "    # optimizers:\n",
        "    sgd=SGD(lr=lr_sgd_rms, momentum=0.9, decay=decay, nesterov=False)  # default: SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "    rmsprop=RMSprop(lr=lr_sgd_rms, rho=0.9, epsilon=None, decay=decay) # default:RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "    adagrad=Adagrad(lr=lr_adagrad, epsilon=None, decay=decay)    # default: Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
        "    adam=Adam(lr=lr_adam, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay, amsgrad=False)   # default=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "    train_algo = adam #  rd.choice([sgd, rmsprop, adagrad, adam])\n",
        "    \n",
        "    scaling = 2   # rd.choice([0, 1, 2])   # 0 = No scaling | 1 = MinMax Scaling | 2 = Z-Score \n",
        "    preparation = 1 # rd.choice([1, 2])\n",
        "    val_split = rd.choice([0.25, 0.3, 0.35])\n",
        "    train_days = rd.choice([ 5, 7, 9])\n",
        "    activation = 'relu' # rd.choice([ 'sigmoid', 'relu', 'tanh'])\n",
        "    patience = rd.choice([ 10, 20])\n",
        "    batch_size = rd.choice([50, 100, 150, 200, 250, 300])\n",
        "    regularisation = rd.choice([0, 0.001, 0.01, 0.1])\n",
        "    dropout = rd.choice([0.1, 0.2 , 0.3])\n",
        "\n",
        "  # 1.3 Scaling\n",
        "\n",
        "  # Scaling the data between min and max: x_scaled = (x - min(x)) / ( max(x) - min(x)) only with parameters calculated from the train set\n",
        "  if (scaling == 1):\n",
        "      min, max = -1, 1\n",
        "      min_load_train, max_load_train = np.max(data['load']['2014-01-01 00:00:00':'2015-12-31 23:00:00']), np.min(data['load']['2014-01-01 00:00:00':'2015-12-31 23:00:00'])\n",
        "      data['load'] = ((data['load'] - min_load_train) / (max_load_train - min_load_train)) * (max - min) + min    # Inverse scaling: x = (x_scaled - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "      min_temp_train, max_temp_train = np.max(data['temp_72']['2014-01-01 00:00:00':'2015-12-31 23:00:00']), np.min( data['temp_72']['2014-01-01 00:00:00':'2015-12-31 23:00:00'])\n",
        "      data['temp_72'] = ((data['temp_72'] - min_temp_train) / (max_temp_train - min_temp_train)) * (max - min) + min  # Inverse scaling: x = (x_scaled - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "\n",
        "  # Scaling the data as folllows:  x_scaled = (x - mean(x)) / st(x) only with parameters calculated from the train set\n",
        "  if (scaling == 2):\n",
        "      mean_load_train, sd_load_train = np.mean(data['load']['2014-01-01 00:00:00':'2015-12-31 23:00:00']), np.sqrt(np.var(data['load']['2014-01-01 00:00:00':'2015-12-31 23:00:00']))\n",
        "      data['load'] = (data['load'] - mean_load_train) / sd_load_train   # Inverse scaling: x = x_scaled * sd_load_train + mean_load_train\n",
        "      mean_temp_train, sd_temp_train = np.mean(data['temp_72']['2014-01-01 00:00:00':'2015-12-31 23:00:00']), np.sqrt(np.var(data['temp_72']['2014-01-01 00:00:00':'2015-12-31 23:00:00']))\n",
        "      data['temp_72'] = (data['temp_72'] - mean_temp_train) / sd_temp_train  # Inverse scaling: x = x_scaled * sd_load_train + mean_load_train\n",
        "\n",
        "\n",
        "  data = data[['load','temp_72']]       # chose columns to include in forecast (26304, 3) from 2014-01-01 00:00:00 until 2016-12-31 23:00:00\n",
        "\n",
        "  # ensure all data is float\n",
        "  values = data.values.astype('float32')\n",
        "\n",
        "  # specify the number of lag hours\n",
        "  train_n = 24*train_days\n",
        "  predict_n = 24*3\n",
        "  n_features = data.shape[1]   # number of input variables\n",
        "\n",
        "\n",
        "  # split into train and test sets\n",
        "  train_samples = round(365*2*24*(1-val_split))\n",
        "  val_samples = 365*2*24\n",
        "  train = series_to_supervised(values[:train_samples, :],train_n, predict_n).values\n",
        "  val = series_to_supervised(values[train_samples:val_samples, :],train_n, predict_n).values\n",
        "  test = series_to_supervised(values[val_samples:, :],train_n, predict_n).values\n",
        "\n",
        "  # split into input and outputs\n",
        "  input_cols = train_n * n_features    # columns of input tensor\n",
        "  X_train, y_train = train[:, :input_cols], train[:, -predict_n:]  # (training samples, columns of input tensor) | (training samples, number of periods to predict)\n",
        "  X_val, y_val = val[:, :input_cols], val[:, -predict_n:]  # (training samples, columns of input tensor) | (training samples, number of periods to predict)\n",
        "  X_test, y_test = test[:, :input_cols], test[:, -predict_n:]  # (testing samples, columns of input tensor) | (testing samples samples, number of periods to predict)\n",
        "\n",
        "\n",
        "  # Selecting only every j-th sample\n",
        "  j = 1   # j = 24 means you only consider every 24th sample\n",
        "  X_train, y_train = np.array(pd.DataFrame(X_train).iloc[::j, :]), np.array(pd.DataFrame(y_train).iloc[::j, :])\n",
        "  X_test, y_test = np.array(pd.DataFrame(X_test).iloc[::j, :]), np.array(pd.DataFrame(y_test).iloc[::j, :])\n",
        "\n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "  X_train = X_train.reshape((X_train.shape[0], train_n, n_features))  # (training samples, number of periods to train, number of input variables)\n",
        "  X_test = X_test.reshape((X_test.shape[0], train_n, n_features)) # # (testing samples, number of periods to train, number of input variables)\n",
        "  X_val = X_val.reshape((X_val.shape[0], train_n, n_features))  # (training samples, number of periods to train, number of input variables)\n",
        "\n",
        "  ##### 02 Network selection\n",
        "  \n",
        "  ##### 02 Network selection\n",
        "  hiddenNeurons = 160\n",
        "\n",
        "  # architecture\n",
        "  mv_lstm = Sequential()\n",
        "  mv_lstm.add(LSTM(units=hiddenNeurons, activation=activation, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True, \n",
        "                   recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='normal', recurrent_initializer='orthogonal', \n",
        "                   bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None,\n",
        "                   activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, \n",
        "                   recurrent_dropout=0.0, implementation=1, return_state=False, go_backwards=False, stateful=False, unroll=False))\n",
        "  mv_lstm.add(Dropout(0.2))\n",
        "\n",
        "  mv_lstm.add(LSTM(units=hiddenNeurons, activation=activation, return_sequences=False, \n",
        "                   recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='normal', recurrent_initializer='orthogonal', \n",
        "                   bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None,\n",
        "                   activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, \n",
        "                   recurrent_dropout=0.0, implementation=1, return_state=False, go_backwards=False, stateful=False, unroll=False))\n",
        "  mv_lstm.add(Dropout(0.2))\n",
        "\n",
        "  mv_lstm.add(Dense(units=predict_n, activation='linear', kernel_initializer=RandomUniform(minval=-0.05, maxval=0.05)))\n",
        "\n",
        "  # compilation\n",
        "  mv_lstm.compile(optimizer=train_algo, loss='mse') \n",
        "\n",
        "  # patient early stopping\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
        "\n",
        "  # fit model\n",
        "  history = mv_lstm.fit(X_train, y_train, batch_size = batch_size, epochs=100, verbose=2, \n",
        "                        validation_data=(X_val, y_val),callbacks=[es], shuffle=True)\n",
        "\n",
        "  # prediction\n",
        "  if (scaling == 0):\n",
        "      yhat = mv_lstm.predict(X_test)\n",
        "      yfit = mv_lstm.predict(X_train)  # fitted values\n",
        "      yval = mv_lstm.predict(X_val)  # fitted values\n",
        "\n",
        "  if (scaling == 1):\n",
        "      yhat_s = mv_lstm.predict(X_test)\n",
        "      yhat = (yhat_s - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "      y_test = (y_test - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "\n",
        "      yval_s = mv_lstm.predict(X_val)\n",
        "      yval = (yval_s - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "      y_val = (y_val - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "\n",
        "      yfit_s = mv_lstm.predict(X_train) \n",
        "      yfit = (yfit_s - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "      y_train = (y_train - min) / (max - min) * (max_load_train - min_load_train) + min_load_train\n",
        "\n",
        "  if (scaling == 2):\n",
        "      yhat_s = mv_lstm.predict(X_test)\n",
        "      yhat = yhat_s * sd_load_train + mean_load_train\n",
        "      y_test = y_test * sd_load_train + mean_load_train\n",
        "\n",
        "      yfit_s = mv_lstm.predict(X_train)\n",
        "      yfit = yfit_s * sd_load_train + mean_load_train\n",
        "      y_train = y_train  * sd_load_train + mean_load_train\n",
        "\n",
        "      yval_s = mv_lstm.predict(X_val)\n",
        "      yval = yval_s * sd_load_train + mean_load_train\n",
        "      y_val = y_val  * sd_load_train + mean_load_train\n",
        "\n",
        "\n",
        "\n",
        "  # Specification\n",
        "  print('Optimising: ' + str(optimising))\n",
        "  \n",
        "  # evaluation measures\n",
        "  rmse_all = np.sqrt(mean_squared_error(y_test,yhat)); print('rmse_all: ' + str(rmse_all))\n",
        "  rmse_val = np.sqrt(mean_squared_error(y_val,yval)); print('rmse_val: ' + str(rmse_val))\n",
        "  rmse_fit = np.sqrt(mean_squared_error(y_train,yfit)); print('rmse_fit: ' + str(rmse_fit))\n",
        "  time_net = dt.datetime.now() - startNet; print ('time_net: ' + str(time_net))  # observe runtime of single neural net\n",
        "  \n",
        "  if (optimising == 1):\n",
        "    result['rmse_all'][i] = rmse_all\n",
        "    result['rmse_val'][i] = rmse_val\n",
        "    result['rmse_fit'][i] = rmse_fit\n",
        "    result['train_days'][i] = train_days\n",
        "    result['batch_size'][i] = batch_size\n",
        "    if(train_algo == sgd ):\n",
        "      algo = 'sgd'\n",
        "    if(train_algo == rmsprop):\n",
        "      algo = 'rmsprop'\n",
        "    if(train_algo == adagrad):\n",
        "      algo = 'adagrad'\n",
        "    if(train_algo == adam):\n",
        "      algo ='adam'\n",
        "    result['train_algo'][i] = algo    \n",
        "    if(train_algo == sgd or train_algo == rmsprop):\n",
        "      lr = lr_sgd_rms\n",
        "    if(train_algo == adagrad):\n",
        "      lr = lr_adagrad\n",
        "    if(train_algo == adam):\n",
        "      lr = lr_adam\n",
        "    result['lr'][i] = lr\n",
        "    result['decay'][i] = decay\n",
        "    result['patience'][i] = patience \n",
        "    result['val_split'][i] = val_split\n",
        "    result['activation'][i] = activation   \n",
        "    result['duration'][i] = time_net \n",
        "    result['scaling'][i] = scaling\n",
        "    result['regularisation'][i] = regularisation \n",
        "    result['dropout'][i] = dropout\n",
        "    result['epochs'][i] = es.stopped_epoch\n",
        "    result\n",
        "\n",
        "time_script = dt.datetime.now() - startScript; print ('time_script: ' + str(time_script))  # observe runtime of whole script"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFR8Lr1_zTGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### 03 Evaluation\n",
        "\n",
        "# plot history\n",
        "plt.plot(history.history['loss'][10:], label='train')\n",
        "plt.plot(history.history['val_loss'][10:], label='test')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXrsVqEXzrtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot prediction\n",
        " for i in range(0, len(y_test), 24*7*4):\n",
        "     plt.figure(i)\n",
        "     plt.plot(yhat[i], label='prediction')\n",
        "     plt.plot(y_test[i], label ='observation')\n",
        "     plt.title('prediction of week ' + str((round(i/(24*7)+1,0))))\n",
        "     plt.xlabel('hour');plt.ylabel('MW');plt.legend()\n",
        "     plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmP1VqM3zs43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot \"fitted values\"\n",
        " for i in range(0, len(y_train), 24*7*4):\n",
        "     plt.figure(i+10)\n",
        "     plt.plot(yfit[i], label='fitted values')\n",
        "     plt.plot(y_train[i], label ='observation')\n",
        "     plt.title('fitted values of week ' + str((round(i/(24*7)+1,0))))\n",
        "     plt.xlabel('hour');plt.ylabel('MW');plt.legend()\n",
        "     plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}